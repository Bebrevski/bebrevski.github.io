<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>SoftUni Blog</title>
    <link rel="stylesheet" href="styles/style.css">
    <script src="scripts/jquery-1.10.2.js"></script>
    <script src="scripts/bootstrap.js"></script>
</head>
<body>

<header>
    <div class="navbar navbar-default navbar-fixed-top text-uppercase">
        <div class="container">
            <div class="navbar-header">
                <a href="index.html" class="navbar-brand">SoftUni Blog</a>
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
            </div>
            <div class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="###">Register</a></li>
                    <li><a href="###">Login</a></li>
                </ul>
            </div>
        </div>
    </div>
</header>

<main>
    <div class="container body-content">
        <div class="row">
            <div class="col-md-12">
                <article>
                    <header>
                        <h2>Interpretations</h2>
                    </header>

                    <p>
                        Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.
                        The universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.
                        In 1989, the first proof was published by Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Hornik.
                        The probabilistic interpretation derives from the field of machine learning.
                        It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively.
                        More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.
                        The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.
                        The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.
                    </p>

                    <small class="author">
                        SSB
                    </small>

                    <footer>
                        <div class="pull-right">
                            <a class="btn btn-default btn-xs" href="index.html">back &raquo;</a>
                        </div>
                    </footer>
                </article>
            </div>
        </div>
    </div>
</main>

<footer>
    <div class="container modal-footer">
        <p>&copy; 2016 - Software University Foundation</p>
    </div>
</footer>

</body>
</html>