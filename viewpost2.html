<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>SoftUni Blog</title>
    <link rel="stylesheet" href="styles/style.css">
    <script src="scripts/jquery-1.10.2.js"></script>
    <script src="scripts/bootstrap.js"></script>
</head>
<body>

<header>
    <div class="navbar navbar-default navbar-fixed-top text-uppercase">
        <div class="container">
            <div class="navbar-header">
                <a href="index.html" class="navbar-brand">SoftUni Blog</a>
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
            </div>
            <div class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="###">Register</a></li>
                    <li><a href="###">Login</a></li>
                </ul>
            </div>
        </div>
    </div>
</header>

<main>
    <div class="container body-content">
        <div class="row">
            <div class="col-md-12">
                <article>
                    <header>
                        <h2>Concepts</h2>
                    </header>

                    <p>
                        The assumption underlying distributed representations is that observed data are generated by the interactions of layered factors.
                        Deep learning adds the assumption that these layers of factors correspond to levels of abstraction or composition. Varying numbers of layers and layer sizes can provide different degrees of abstraction.
                        Deep learning exploits this idea of hierarchical explanatory factors where higher level, more abstract concepts are learned from the lower level ones.
                        Deep learning architectures are often constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features are useful for improving performance.
                        For supervised learning tasks, deep learning methods obviate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.
                        Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors and deep belief networks.
                    </p>

                    <small class="author">
                        SSB
                    </small>

                    <footer>
                        <div class="pull-right">
                            <a class="btn btn-default btn-xs" href="index.html">back &raquo;</a>
                        </div>
                    </footer>
                </article>
            </div>
        </div>
    </div>
</main>

<footer>
    <div class="container modal-footer">
        <p>&copy; 2016 - Software University Foundation</p>
    </div>
</footer>

</body>
</html>