<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>SoftUni Blog</title>
    <link rel="stylesheet" href="styles/style.css">
    <script src="scripts/jquery-1.10.2.js"></script>
    <script src="scripts/bootstrap.js"></script>
</head>
<body>

    <header>
        <div class="navbar navbar-default navbar-fixed-top text-uppercase">
            <div class="container">
                <div class="navbar-header">
                    <a href="index.html" class="navbar-brand">SoftUni Blog</a>
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>
                <div class="navbar-collapse collapse">
                    <ul class="nav navbar-nav navbar-right">
                        <li><a href="registerpage.html">Register</a></li>
                        <li><a href="loginpage.html">Login</a></li>
                    </ul>
                </div>
            </div>
        </div>
    </header>

    <main>
        <div class="container body-content">
            <div class="row">
                <div class="col-md-6">
                    <article>
                        <header>
                            <h2>Deep learning</h2>
                        </header>

                        <p>
                            Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms.
                            Learning can be supervised, partially supervised or unsupervised.
                            Some representations are loosely based on interpretation of information processing and communication patterns in a biological nervous system, such as neural coding that attempts to define a relationship between various stimuli and associated neuronal responses in the brain.
                            Research attempts to create efficient systems to learn these representations from large-scale, unlabeled data sets.
                            Deep learning architectures such as deep neural networks, deep belief networks and recurrent neural networks have been applied to fields including computer
                            vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics and drug design, where they produced results comparable to and in some cases superior to human experts.
                        </p>

                        <small class="author">
                        SSB
                        </small>

                        <footer>
                            <div class="pull-right">
                                <a class="btn btn-default btn-xs" href="viewpost.html">Read more &raquo;</a>
                            </div>
                        </footer>
                    </article>
                </div>
                <div class="col-md-6">
                    <article>
                        <header>
                            <h2>Definitions</h2>
                        </header>

                        <p>
                            Deep learning is a class of machine learning algorithms that:
                            use a cascade of multiple layers of nonlinear processing units for feature extraction and transformation. Each successive layer uses the output from the previous layer as input.
                            learn in supervised (e.g., classification) and/or unsupervised (e.g., pattern analysis) manners.
                            learn multiple levels of representations that correspond to different levels of abstraction; the levels form a hierarchy of concepts.
                            use some form of gradient descent for training via backpropagation.
                            Layers that have been used in deep learning include hidden layers of an artificial neural network and sets of propositional formulas.[10] They may also include latent variables organized layer-wise in deep generative models such as the nodes in Deep Belief Networks and Deep Boltzmann Machines.
                        </p>

                        <small class="author">
                            SSB
                        </small>

                        <footer>
                            <div class="pull-right">
                                <a class="btn btn-default btn-xs" href="viewpost1.html">Read more &raquo;</a>
                            </div>
                        </footer>
                    </article>
                </div>
                <div class="col-md-6">
                    <article>
                        <header>
                            <h2>Concepts</h2>
                        </header>

                        <p>
                            The assumption underlying distributed representations is that observed data are generated by the interactions of layered factors.
                            Deep learning adds the assumption that these layers of factors correspond to levels of abstraction or composition. Varying numbers of layers and layer sizes can provide different degrees of abstraction.
                            Deep learning exploits this idea of hierarchical explanatory factors where higher level, more abstract concepts are learned from the lower level ones.
                            Deep learning architectures are often constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features are useful for improving performance.
                            For supervised learning tasks, deep learning methods obviate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.
                            Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors and deep belief networks.
                        </p>

                        <small class="author">
                            SSB
                        </small>

                        <footer>
                            <div class="pull-right">
                                <a class="btn btn-default btn-xs" href="viewpost2.html">Read more &raquo;</a>
                            </div>
                        </footer>
                    </article>
                </div>
                <div class="col-md-6">
                    <article>
                        <header>
                            <h2>Interpretations</h2>
                        </header>

                        <p>
                            Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.
                            The universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.
                            In 1989, the first proof was published by Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Hornik.
                            The probabilistic interpretation derives from the field of machine learning.
                            It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively.
                            More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.
                            The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.
                            The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.
                        </p>

                        <small class="author">
                            SSB
                        </small>

                        <footer>
                            <div class="pull-right">
                                <a class="btn btn-default btn-xs" href="viewpost3.html">Read more &raquo;</a>
                            </div>
                        </footer>
                    </article>
                </div>
            </div>
        </div>
    </main>

    <footer>
        <div class="container modal-footer">
            <p>&copy; 2016 - Software University Foundation</p>
        </div>
    </footer>

</body>
</html>